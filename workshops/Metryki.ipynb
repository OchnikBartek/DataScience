{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 📘 Notebook: Ewaluacja modeli ML – AI Warsztaty"
      ],
      "metadata": {
        "id": "YooHlm7ObBWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧱 Spis treści\n",
        "\n",
        "1. Wprowadzenie – po co ewaluować modele?\n",
        "\n",
        "2. Klasyfikacja\n",
        "\n",
        "3. Regresja\n",
        "\n",
        "4. Grupowanie (unsupervised)\n",
        "\n",
        "5. Sieci neuronowe (specyfika ewaluacji)\n",
        "\n",
        "6. Walidacja modeli i overfitting\n",
        "\n",
        "7. Cheat Sheet – skróty, wybór metryk"
      ],
      "metadata": {
        "id": "yOGfjgQzbFGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. 🧠 Po co ewaluować modele?"
      ],
      "metadata": {
        "id": "LCiAV8jMbKo8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Czy Twój model działa dobrze?\n",
        "\n",
        "- Ewaluacja modelu to nie tylko sprawdzenie \"czy działa\", ale:\n",
        "  - Czy uczy się właściwego wzorca?\n",
        "  - Czy generalizuje?\n",
        "  - Czy jest lepszy niż losowe zgadywanie?\n",
        "\n",
        "🔎 Dzięki ewaluacji:\n",
        "- porównujemy modele i wybieramy najlepszy\n",
        "- unikamy pułapek typu overfitting\n",
        "- dobieramy odpowiednie metody do konkretnego problemu\n"
      ],
      "metadata": {
        "id": "D4Ms9vTVbNXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. 📊 Klasyfikacja"
      ],
      "metadata": {
        "id": "bZBfcQoFbQX5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Co to jest klasyfikacja?\n",
        "\n",
        "To przewidywanie jednej z kilku klas (np. TAK/NIE, kot/pies, pozytywny/negatywny).\n",
        "\n",
        "📌 Przykłady:\n",
        "- Email to spam / nie spam\n",
        "- Czy pacjent ma raka\n",
        "- Czy klient spłaci kredyt\n",
        "\n",
        "---\n",
        "\n",
        "### 📏 Metryki klasyfikacyjne\n",
        "\n",
        "1. **Accuracy** – % poprawnych odpowiedzi\n",
        "   - Działa tylko przy zrównoważonych klasach\n",
        "   - Nie sprawdza się przy danych typu: 95% klasy \"NIE\"\n",
        "\n",
        "2. **Precision** – spośród przewidzianych jako pozytywne, ile było faktycznie pozytywnych  \n",
        "   > Wysoka precision: mało fałszywych alarmów\n",
        "\n",
        "3. **Recall** – spośród faktycznie pozytywnych, ile model wykrył  \n",
        "   > Wysoki recall: mało przypadków przeoczonych\n",
        "\n",
        "4. **F1-score** – balans precision i recall\n",
        "\n",
        "5. **AUC-ROC** – ocena zdolności odróżnienia klas (niezależnie od progu)\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 Kiedy używać której?\n",
        "\n",
        "| Problem | Ważna metryka |\n",
        "|--------|----------------|\n",
        "| Zbalansowane dane | Accuracy, F1 |\n",
        "| Niezbalansowane dane | F1, Precision, Recall |\n",
        "| Bezpieczeństwo, zdrowie | Recall |\n",
        "| Blokowanie spamu | Precision |\n",
        "| Wynik to prawdopodobieństwo | AUC-ROC |\n"
      ],
      "metadata": {
        "id": "dAsUgNw-bg3R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ćwiczenie 1: Wytrenuj klasyfikator na zbiorze breast_cancer\n",
        "\n",
        "1) Naucz się obliczać accuracy, precision, recall i F1."
      ],
      "metadata": {
        "id": "N9iqfVOXczSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, stratify=data.target)\n",
        "\n",
        "model = LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "QUUIiPDCc89w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ćwiczenie 2: Zinterpretuj wyniki\n",
        "\n",
        "1) Kiedy model bardziej myli się (fałszywe pozytywy czy negatywy)?\n",
        "\n",
        "2) Co to może znaczyć w kontekście diagnozy medycznej?"
      ],
      "metadata": {
        "id": "VAdPGSxzdAn0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. 📈 Regresja"
      ],
      "metadata": {
        "id": "pSKb4VZ2bqap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Czym jest regresja?\n",
        "\n",
        "Model przewiduje wartość liczbową:\n",
        "- prognoza cen\n",
        "- przewidywanie temperatury\n",
        "- szacowanie ryzyka\n",
        "\n",
        "---\n",
        "\n",
        "### 📏 Metryki regresji\n",
        "\n",
        "1. **MAE (Mean Absolute Error)**  \n",
        "   - Średni błąd bezwzględny\n",
        "   - Intuicyjny: \"średnio się pomyliłem o X\"\n",
        "\n",
        "2. **MSE (Mean Squared Error)**  \n",
        "   - Bardziej karze duże błędy\n",
        "\n",
        "3. **RMSE (Root MSE)**  \n",
        "   - MSE znormalizowane – ta sama jednostka co target\n",
        "\n",
        "4. **R² (coefficient of determination)**  \n",
        "   - Wskaźnik, jak dobrze model tłumaczy zmienność danych (0–1)\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 Kiedy używać której?\n",
        "\n",
        "| Problem | Metryka |\n",
        "|--------|---------|\n",
        "| Błąd ma być \"ludzki\" | MAE |\n",
        "| Chcesz karać duże błędy | MSE / RMSE |\n",
        "| Chcesz ocenić ogólną jakość dopasowania | R² |\n"
      ],
      "metadata": {
        "id": "8ZC1hs-Pbrud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ćwiczenie 1: Porównanie MAE i RMSE\n",
        "\n",
        "1) Zobacz różnicę między karaniem dużych błędów."
      ],
      "metadata": {
        "id": "NVJGo3YpdP14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "print(\"MAE:\", mae)\n",
        "print(\"RMSE:\", rmse)\n"
      ],
      "metadata": {
        "id": "Pmk7GvWIdSe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ćwiczenie 2: Stwórz wykres błędów\n",
        "\n",
        "1) Narysuj histogram błędów predykcji (y_test - y_pred)\n",
        "\n",
        "2) Co możesz z niego wyczytać?"
      ],
      "metadata": {
        "id": "SBBn1X2udTlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. 🔍 Grupowanie (Clustering)"
      ],
      "metadata": {
        "id": "_qMbXgIqbuJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Brak etykiet – jak ocenić?\n",
        "\n",
        "W grupowaniu (np. KMeans) nie mamy \"prawdziwych\" odpowiedzi.  \n",
        "Model sam szuka podobieństw – musimy ocenić jakość klastrów.\n",
        "\n",
        "---\n",
        "\n",
        "### 📏 Metryki unsupervised:\n",
        "\n",
        "1. **Silhouette Score**  \n",
        "   - Miara spójności i separacji klas  \n",
        "   - od -1 do 1 (im wyżej, tym lepiej)\n",
        "\n",
        "2. **Calinski-Harabasz Index**  \n",
        "   - Stosunek odległości między klastrami do wewnętrznej spójności  \n",
        "   - im wyższy, tym lepiej\n",
        "\n",
        "3. **Davies-Bouldin Index**  \n",
        "   - Niższy = lepiej\n",
        "\n",
        "---\n",
        "\n",
        "### 📌 Co robić?\n",
        "\n",
        "- Próbować różnych K i porównywać metryki\n",
        "- Wizualizować: PCA, t-SNE\n"
      ],
      "metadata": {
        "id": "lytSAh83bwaV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ćwiczenie 1: KMeans + silhouette score\n",
        "\n",
        "1) Naucz się oceniać jakość klastrów"
      ],
      "metadata": {
        "id": "d-6ldq_RdsVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "X, _ = make_blobs(n_samples=300, centers=4, random_state=42)\n",
        "\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "labels = kmeans.fit_predict(X)\n",
        "\n",
        "score = silhouette_score(X, labels)\n",
        "print(\"Silhouette Score:\", score)\n"
      ],
      "metadata": {
        "id": "vkNixSMBdwPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ćwiczenie 2: Zmień liczbę klastrów\n",
        "\n",
        "1) Sprawdź, jak zmienia się wynik silhouette_score\n",
        "\n",
        "2) Czy zawsze więcej klastrów = lepiej?"
      ],
      "metadata": {
        "id": "OxYYZnRmdydM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 🧠 Sieci neuronowe"
      ],
      "metadata": {
        "id": "_t-1EGXibzRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sieci uczą się na wiele epok – trzeba kontrolować postęp\n",
        "\n",
        "📏 Monitorujemy:\n",
        "- **Loss (funkcja straty)** na train i validation\n",
        "- Accuracy/F1 na validation\n",
        "- Early stopping – zatrzymaj trening gdy val_loss rośnie\n",
        "\n",
        "📊 Często wykresy:\n",
        "- loss vs epoch\n",
        "- val_loss vs epoch\n",
        "\n",
        "Używamy callbacków np. w TensorFlow:\n",
        "```python\n",
        "EarlyStopping(monitor='val_loss', patience=3)\n"
      ],
      "metadata": {
        "id": "-sjWgGX-b2lg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ćwiczenie 1: Ucz sieć na MNIST i rysuj loss\n",
        "\n",
        "1) Zobacz overfitting i użycie early stopping"
      ],
      "metadata": {
        "id": "91Vg8NWKd23Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "QZv6bF9o3ZEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', patience=3)\n",
        "history = model.fit(X_train, y_train, epochs=20, validation_split=0.2, callbacks=[es])\n"
      ],
      "metadata": {
        "id": "EBUjUfgId8lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ćwiczenie 2: Wykres loss i accuracy"
      ],
      "metadata": {
        "id": "87ycQ14ld-A_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'], label='train loss')\n",
        "plt.plot(history.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.title('Loss vs Val Loss')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "33y01Fq5eAeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. 🧪 Walidacja i overfitting"
      ],
      "metadata": {
        "id": "CoVrN6mxcjWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Podstawy:\n",
        "\n",
        "- **Train/test split** – dzielimy dane na trening i test\n",
        "- **Cross-validation** – np. K-fold\n",
        "- **StratifiedKFold** – zachowuje proporcje klas\n",
        "\n",
        "📌 Overfitting = model za bardzo uczy się danych treningowych  \n",
        "➡️ Działa świetnie na train, źle na test\n",
        "\n",
        "### Jak to rozpoznać?\n",
        "\n",
        "- accuracy na train: 99%, na test: 60% → źle\n",
        "- rosnący train_loss, rosnący val_loss → źle"
      ],
      "metadata": {
        "id": "1nB23umncn9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ćwiczenie 1: Cross-validation w klasyfikacji"
      ],
      "metadata": {
        "id": "PfxNbqlreZHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_train ndim:\", X_train.ndim)"
      ],
      "metadata": {
        "id": "WqmbFhis45R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Twoje dane mają kształt (60000, 28, 28), czyli 60 tys. obrazków 28x28 pikseli – wygląda na klasyczny zbiór MNIST albo coś podobnego. To są dane obrazowe, a RandomForestClassifier nie działa bezpośrednio na obrazach 2D – trzeba je najpierw spłaszczyć do postaci 2D."
      ],
      "metadata": {
        "id": "k54ONm425Cy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Spłaszczenie danych: (60000, 28, 28) → (60000, 784)\n",
        "X_train_flat = X_train.reshape(X_train.shape[0], -1)"
      ],
      "metadata": {
        "id": "tpm935Gk5CLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')\n",
        "\n",
        "print(\"F1-scores:\", scores)\n",
        "print(\"Średni F1:\", scores.mean())\n"
      ],
      "metadata": {
        "id": "j8855-8oeZqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ćwiczenie 2: StratifiedKFold na niezbalansowanym zbiorze\n",
        "\n",
        "1) Zrób eksperyment: weź zbiór z niezbalansowanymi klasami i porównaj zwykły KFold vs StratifiedKFold.\n",
        "\n",
        "2) Jakie są różnice w wynikach?"
      ],
      "metadata": {
        "id": "phtJI8GEeJm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. 🧠 Cheat Sheet – metryki"
      ],
      "metadata": {
        "id": "66eyOoVmb6_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Problem         | Metryki                   |\n",
        "|----------------|---------------------------|\n",
        "| Klasyfikacja   | Accuracy, F1, Precision, Recall, AUC |\n",
        "| Regresja       | MAE, MSE, RMSE, R²         |\n",
        "| Clustering     | Silhouette, Calinski-Harabasz, Davies-Bouldin |\n",
        "| Sieci neuronowe | Loss, val_loss, val_acc     |\n"
      ],
      "metadata": {
        "id": "U4j_MUqdb9Wd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. 💼 Projekt – przykładowa ewaluacja"
      ],
      "metadata": {
        "id": "IgiCnBUYb-el"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zobacz, że wyżej,: ten sam zbiór danych, ten sam problem, zrobiłeś za pomocą różnych modeli, o których mówiliśmy na warsztatach. Możesz zobaczyć, że analiza tematu i to w jaki sposób podejdziesz do tematu jest kluczowym elementem każdego projektu."
      ],
      "metadata": {
        "id": "MmgWh2nu67Ty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📘 Dane: np. predykcja choroby (klasyfikacja binarna)\n",
        "\n",
        "1. Wczytaj dane (z CSV)\n",
        "2. Podziel na train/test\n",
        "3. Trenuj np. Logistic Regression\n",
        "4. Oblicz metryki: accuracy, precision, recall, F1\n",
        "5. Dodaj wykresy (np. confusion matrix)\n",
        "6. BONUS: zrób cross-validation i porównaj metryki\n",
        "\n",
        "📎 Tip: możesz użyć `classification_report` ze sklearn\n"
      ],
      "metadata": {
        "id": "X7KIPRc7cCr4"
      }
    }
  ]
}